@startuml
title File Upload Process Clean (First Upload)
actor User as user 
participant "Front End" as frontend
database "local storage" as storage
participant "S3 Bucket" as bucket
participant "dynamoDb" as dynamo
participant "<<lambda>>\nFile Ingestion" as fileIngestion
participant "Athena" as athena

frontend->dynamo : get view state for project
note right 
 A view is composed of a combination of tables.
 A table is composed of a combination of files with 
 the same columns and types (do they need to be in 
 the same order?)
end note
user-> frontend: select file
group "in paralell" 
frontend->frontend : determine column types
frontend->frontend : calculate statistics
frontend->frontend : compare statistics
note right 
a table name is defined as the basename of 
the table.  i.e. Table1.csv will have a table 
name of Table1.  Each inbound file will be 
compared against all of the files in a table 
set.
end note
end

group if the inbound table is the same shape \nas the other tables but it has \ndiffernt data than the files in the table set.
frontend->user: append data or everwrite? 
note left 
  chosing to append data will add 
  a new file to the tableset.  
  For example, if we have a table
  named Table1 with an existing file 
  Table1.csv in its file set.  
  Appending the data will and Table1-1.csv 
  to its table set, and the data will be 
  combined together.
end note

end

group Adding a new table 
frontend->frontend: render grid(using local data)
else Existing table 
	frontend -> bucket: get table set 
	frontend -> frontend  : render grid (with bucket files)
end

frontend->fileIngestion: process file(modelId, bucketName, file statistic, \nfileData: {\n  tableName: string, \n  fileName: string, \n  Operation: Add | Replace | Append | Delete,  \n  fileStream?:Readable\n}[])

group forEach .csv file in fileData 

group if operation===Add
fileIngestion->bucket : write csv \n(client/clientId/projectId/input/tablename/fileName.csv)
fileIngestion->bucket : write parquet\n(client/clientId/projectId/data/tablename/fileName.parquet) 
fileIngestion->athena : create table.

else if operation===Replace 
fileIngestion->athena : delete table 
fileIngestion->athena : delete view 
note right 
	once the backing data is removed, 
	the view is broken.
end note
fileIngestion->bucket : archive existing csv and parquet files (we still want the data for analysis)
fileIngestion->bucket : write csv \n(client/clientId/projectId/input/tablename/fileName.csv)
fileIngestion->bucket : write parquet\n(client/clientId/projectId/data/tablename/fileName.parquet) 
fileIngestion->athena : create table.
else if operation===Append
fileIngestion->bucket : write csv \n(client/clientId/projectId/input/tablename/fileName.csv)
note left 
	this will add a new file with 
	a unique name to the fileset 
	i.e. table1-1.csv
end note
fileIngestion->bucket : write parquet\n(client/clientId/projectId/data/tablename/fileName.parquet) 
note left 
Adding parquet files to our 
set will simply make it available
to the view
end note

else if operation===Delete
fileIngestion->athena : delete table 
fileIngestion->athena : delete view 
fileIngestion->bucket : archive existing csv and parquet files (we still want the data for analysis)
end
end
group if 1 or more file operations !== Append 
fileIngestion->athena : create view (aggregate of all tables),
end
fileIngestion -> fileIngestion : update file statistics
fileIngestion-->frontend : updated file statistics + errors

frontend->dynamo : save file statistics
@enduml
