@startuml
!theme spacelab

title ETL Process File Load New Process (using views) 

participant Dashboard as app 
participant "S3 Bucket" as s3
participant "ETL Process" as etl <<AWS Lambda>>
participant "AWS Athena" as athena
participant "Elastic File System" as efs <<efs>>

app -> s3 : upload files (2...n)
app-> etl : process uploaded files
group 2...n files
	loop for each file
		etl-> s3 : first 100 rows (should\nstill stream this, first\nrow is column names) 
		group if not first file
			etl -> etl : determine joins to\nfirst file(based on\nshared column names)
		end
		etl->etl : clean column names
		etl->etl : determine column\ntypes (based on\nsample)
		etl->etl : define parquet file
		etl->s3 : stream data (process\nfor first hundred rows\nis included in this)
		loop for each row in file
			etl->etl : clean column data
			alt invalid values
			   etl->efs : write errors to log
			end
			etl->efs : write parquet row
			note right 
		     if we have invalid column data
			   we will just set the value 
			   to NULL, this will allow us
			   to salvage at least some of the 
			   data
			end note
			etl->efs : write csv row
		end
		etl->efs : get parquert stream
		etl->efs : get csv stream
		etl->efs : get error log stream
		
		group paralell
			etl->s3 : upload parquet file (stream)
			etl->s3 : uplaod csv file(stream)
			etl->s3 : upload error log (stream)
		end
		etl->athena : build table\n(from s3 parquet file)
	end
	etl->athena : define view \n (join of all tables processed)
	note right 
		instead of trying to munge
		the files together using 
		our code, we can create an 
		athena view which is 
		essentially a precompiled 
		query.  This gives us a ton 
		of flexibility and takes the 
		burden off of our logic
	end note
end

group 1 file
	etl-> s3 : first 100 rows (should\nstill stream this, first\nrow is column names) 
	etl->etl : clean column names
	etl->etl : determine column\ntypes (based on\nsample)
	etl->etl : define parquet file
	etl->s3 : stream data (process\nfor first hundred rows\nis included in this)
		loop for each row in file
			etl->etl : clean column data
			alt invalid values
			   etl->efs : write errors to log
			end
			etl->efs : write parquet row
			note right 
		     if we have invalid column data
			   we will just set the value 
			   to NULL, this will allow us
			   to salvage at least some of the 
			   data
			end note
			etl->efs : write csv row
		end
		etl->efs : get parquert stream
		etl->efs : get csv stream
		etl->efs : get error log stream
		
		group paralell
			etl->s3 : upload parquet file (stream)
			etl->s3 : uplaod csv file(stream)
			etl->s3 : upload error log (stream)
		end
		etl->athena : build table\n(from s3 parquet file)
end
etl->efs : cleanup intermediate files
etl-->app : return view/table information
note right
from a query perspective
tables and views are 
accessed the same way
end note

@enduml
